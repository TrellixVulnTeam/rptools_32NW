{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24a3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean , stdev\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from rdkit import RDLogger\n",
    "import argparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f83062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_template_df():\n",
    "    '''Creates a template dataframe \n",
    "       containing all the feature columns''' \n",
    "    global no_of_rxns_thres\n",
    "    feature_data_columns = []\n",
    "    for n in range(no_of_rxns_thres):\n",
    "        smarts = \"Rxn\"+str(n+1)+\"_SMARTS\"\n",
    "        rxn_delta_g = \"Rxn\"+str(n+1)+\"_DeltaG\"\n",
    "        rxn_rule_score = \"Rxn\"+str(n+1)+\"_Rule_Score\"\n",
    "        feature_data_columns.extend([smarts, rxn_delta_g, rxn_rule_score])\n",
    "    feature_data_columns.extend([\"Pathway_Delta_G\", \"Pathway_Flux\", \"Pathway_Score\", \"Round1\"])\n",
    "    #print(feature_data_columns)\n",
    "    feature_data = pd.DataFrame(  columns = feature_data_columns , index = None)\n",
    "    return feature_data\n",
    "\n",
    "def loop(i , temp, data):\n",
    "    '''Returns the indices of all the reactions\n",
    "       for a pathway in the dataset''' \n",
    "    temp_list = []\n",
    "    break_index = None \n",
    "    flag = True\n",
    "    j = 1\n",
    "    for index in range(i, len(data)):\n",
    "        if temp == data.loc[index,'Pathway Name'] and data.loc[index,'Reaction'] == \"RP\"+str(j):\n",
    "            j =j+1\n",
    "            temp_list.append(index)\n",
    "            if index+1 == len(data):\n",
    "                flag = False\n",
    "        else:\n",
    "            break_index = index\n",
    "            break\n",
    "    return temp_list , break_index , flag\n",
    "\n",
    "\n",
    "def pathways_index_list(data):\n",
    "    '''Returns the indices of all the reactions\n",
    "       for each of the pathways in dataset''' \n",
    "    pathways_index_list = []\n",
    "    i = 0 \n",
    "    flag = True\n",
    "    while flag:\n",
    "        temp = data.loc[i,'Pathway Name']\n",
    "        temp_list, i , flag = loop( i , temp, data)\n",
    "        pathways_index_list.append(temp_list)\n",
    "\n",
    "    return pathways_index_list\n",
    "\n",
    "\n",
    "def transform_into_pathway_features(data, scores, flag):\n",
    "    '''Generates the dataframe containing\n",
    "       all the features. \n",
    "       data and scores ate the 2 inputs files\n",
    "       The reactions are represented in SMILES'''\n",
    "\n",
    "    global no_of_rxns_thres\n",
    "    df = feature_template_df()\n",
    "    pathways_list = pathways_index_list(data)\n",
    "    #print(pathways_list)\n",
    "    #print(len(pathways_list))\n",
    "    drop_list = []\n",
    "    print(\"Transforming into pathway features...\")\n",
    "    for count_p, rxn_list in tqdm(enumerate(pathways_list)) :\n",
    "        if len(rxn_list) > 10:\n",
    "            drop_list.append(count_p)\n",
    "            continue\n",
    "        # At the level of each reaction reading data file\n",
    "        for n , index in enumerate(rxn_list):\n",
    "            #print(index)\n",
    "            smarts = \"Rxn\"+str(n+1)+\"_SMARTS\"\n",
    "            rxn_delta_g = \"Rxn\"+str(n+1)+\"_DeltaG\"\n",
    "            rxn_rule_score = \"Rxn\"+str(n+1)+\"_Rule_Score\"\n",
    "            df.loc[count_p, smarts] = data.loc[index,'Reaction Rule']\n",
    "            df.loc[count_p, rxn_delta_g] = data.loc[index,'Normalised dfG_prime_m']\n",
    "            df.loc[count_p, rxn_rule_score] = data.loc[index,'Rule Score']\n",
    "        # At the level of pathway reading scores file\n",
    "        df.loc[count_p, \"Pathway_Delta_G\"] = scores.loc [count_p, 'dfG_prime_m'] \n",
    "        df.loc[count_p, \"Pathway_Flux\"] = float( scores.loc[count_p,'FBA Flux'].split(';')[1] ) \n",
    "        df.loc[count_p, \"Pathway_Score\"] = scores.loc[count_p,'Global Score']\n",
    "        df.loc[count_p, \"Lit\"] = scores.loc[count_p,'Lit']\n",
    "        df.loc[count_p, \"Round1\"] = scores.loc[count_p,'Round1']\n",
    "    df = df.drop(drop_list)\n",
    "    df = df.fillna(0)\n",
    "    if flag:\n",
    "        df = df[~(df.Round1 < 0)]\n",
    "        df[\"Round1\"][df['Round1'] > 0] = 1\n",
    "        #df.to_csv(\"raw_df.csv\")\n",
    "        df[\"Round1_OR\"] = df[\"Round1\"]\n",
    "        df = shuffle(df, random_state = 42).reset_index(drop =True)\n",
    "        for row in range(len(df)):\n",
    "            if  df.loc[row , \"Lit\"] == 1 :\n",
    "                df.loc[row , \"Round1_OR\"] = 1\n",
    "        #df.to_csv(\"raw_df_csv\", index = None)  \n",
    "        #print(df)\n",
    "    else :\n",
    "        df[\"Round1_OR\"] = df[\"Round1\"]\n",
    "    return df\n",
    "\n",
    "def features_encoding (df, flag):\n",
    "    '''Creates a HDF5 file containing\n",
    "       all the features\n",
    "       Rnx features are encoded in fingerprints'''\n",
    "    no_of_rxns = 10\n",
    "    fp_len = 4096\n",
    "    rxn_len = fp_len + 2\n",
    "    pathway_len = 3\n",
    "    y_len = 1\n",
    "\n",
    "    if flag == \"train\":\n",
    "        sys.exit('Encoding feature for training data not available file data_train.h5 must be present in models folder')\n",
    "    elif flag == \"predict\":\n",
    "        path = 'models/data_predict.h5'\n",
    "        print(\"Encodining features for the Test set......\")\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    f=h5py.File(path, \"w\")\n",
    "    dset = f.create_dataset('data', (  0, (rxn_len*no_of_rxns + pathway_len + y_len)),dtype='i2',maxshape=(None,(rxn_len*no_of_rxns + pathway_len + y_len)), compression='gzip')\n",
    "\n",
    "    for row in tqdm(range(len(df))):\n",
    "        pathway_rxns = np.array([]).reshape(0, rxn_len * no_of_rxns)\n",
    "        rxns_list = []\n",
    "        for rxn_no_ in range(no_of_rxns):\n",
    "            \n",
    "            rxn_smiles_index = rxn_no_ * 3\n",
    "            rxn_dg_index = (rxn_no_ + 1)* 3 -2\n",
    "            rxn_rule_score_index = (rxn_no_ + 1)* 3 - 1\n",
    "        \n",
    "            if  str(df.iloc[row , rxn_smiles_index]) != '0':\n",
    "                #print(df.iloc[row , rxn_smiles_index])\n",
    "                rxn_smiles = df.iloc[row , rxn_smiles_index]\n",
    "                rxn_smiles_list = rxn_smiles.split(\">>\")\n",
    "                #print(len(rxn_smiles_list))\n",
    "\n",
    "                if len(rxn_smiles_list) == 2:\n",
    "\n",
    "                    sub_smiles = rxn_smiles_list[0]\n",
    "                    sub_m= Chem.MolFromSmiles(sub_smiles)\n",
    "                    #print(m)\n",
    "                    sub_fp = AllChem.GetMorganFingerprintAsBitVect(sub_m, 2, nBits = 2048)\n",
    "                    sub_arr = np.array([])\n",
    "                    DataStructs.ConvertToNumpyArray(sub_fp, sub_arr)\n",
    "                    sub_fp= sub_arr.reshape(1,-1)\n",
    "\n",
    "                    pro_smiles = rxn_smiles_list[1]\n",
    "                    pro_m= Chem.MolFromSmiles(pro_smiles)\n",
    "                    #print(m)\n",
    "                    pro_fp = AllChem.GetMorganFingerprintAsBitVect(pro_m, 2, nBits = 2048)\n",
    "                    pro_arr = np.zeros((1,))\n",
    "                    DataStructs.ConvertToNumpyArray(pro_fp, pro_arr)\n",
    "                    pro_fp= pro_arr.reshape(1,-1)\n",
    "                    rxn_fp = np.concatenate([sub_fp , pro_fp]).reshape(1, -1)\n",
    "\n",
    "                elif len(rxn_smiles_list) < 2:\n",
    "                     \n",
    "                    pro_smiles = rxn_smiles_list[0]\n",
    "                    #print(pro_smiles)\n",
    "                    pro_m= Chem.MolFromSmiles(pro_smiles)\n",
    "                    #print(pro_m)\n",
    "                    pro_fp = AllChem.GetMorganFingerprintAsBitVect(pro_m, 2, nBits = fp_len) # JLF: not good !!\n",
    "                    pro_arr = np.zeros((1,))\n",
    "                    DataStructs.ConvertToNumpyArray(pro_fp, pro_arr)\n",
    "                    rxn_fp= pro_arr.reshape(1,-1)\n",
    "                else:\n",
    "                    print(\"There is a problem with the number of components in the reaction\")\n",
    "\n",
    "            else:\n",
    "                rxn_fp = np.zeros(fp_len).reshape(1,-1)\n",
    "\n",
    "            rxn_dg = df.iloc[row , rxn_dg_index].reshape(1,-1)\n",
    "            rxn_rule_score = df.iloc[row , rxn_rule_score_index].reshape(1,-1)\n",
    "            rxns_list.extend([rxn_fp, rxn_dg, rxn_rule_score])\n",
    "            #print(rxn_rule_score)\n",
    "\n",
    "        pathway_rxns = np.concatenate(rxns_list , axis = 1).reshape(1,-1)\n",
    "        pathway_dg = df.loc[row, \"Pathway_Delta_G\"].reshape(1,-1)\n",
    "        pathway_flux = df.loc[row, \"Pathway_Flux\"].reshape(1,-1)\n",
    "        pathway_score = df.loc[row, \"Pathway_Score\"].reshape(1,-1)\n",
    "        pathway_y = df.loc[row, \"Round1_OR\"].reshape(1,-1)\n",
    "        feature = np.concatenate((pathway_rxns, pathway_dg, pathway_flux, pathway_score, pathway_y), axis =1)\n",
    "        dset.resize(dset.shape[0]+feature.shape[0], axis=0) \n",
    "        dset[-feature.shape[0]:]= feature\n",
    "        #print(pathway_flux)\n",
    "\n",
    "    return dset\n",
    "\n",
    "\n",
    "def transform_to_matrix(dset):\n",
    "    ''''Transforms the prediction dataset into\n",
    "        an appropriate matrix which is suitable for\n",
    "        XGBoost'''\n",
    "    X_test = dset[:,:-1]\n",
    "    Y_test = dset[:, -1]\n",
    "    \n",
    "    if not os.path.exists('models/model.pickle'):\n",
    "        sys.exit('model.pickle not found')\n",
    "    else:\n",
    "        trained_model = pickle.load(open('models/model.pickle' ,'rb'))\n",
    "\n",
    "    dset_matrix = xgb.DMatrix(X_test, label = Y_test)\n",
    "    trained_model_score =  trained_model.predict(dset_matrix)\n",
    "    trained_model_score_1 = trained_model_score[:, 1].reshape(-1,1)\n",
    "    X_test = np.concatenate((X_test, trained_model_score_1), axis = 1)\n",
    "    dset_matrix = xgb.DMatrix(X_test)\n",
    "\n",
    "    return  dset_matrix\n",
    "    \n",
    "\n",
    "###############################################################\n",
    "def score_prediction(features_dset_train, features_dset_pred):\n",
    "\n",
    "    stdev_ = []\n",
    "    mean_ = []\n",
    "    pb1_mean = []\n",
    "    pb1_stdev = []\n",
    "    print(features_dset_pred)\n",
    "    n_predictions = np.array([[]])\n",
    "    print(\"Predicting n times...\")\n",
    "    for model_number,  n in tqdm( enumerate([ 0, 10, 20, 30, 40, 50, 60, 70,80, 90])): ##########################\n",
    "        modelfile = \"models/model\" + str(model_number )+ \".pickle\"\n",
    "        if not os.path.exists(modelfile):\n",
    "            sys.exit('modelfile not found')\n",
    "        model = pickle.load( open(modelfile, 'rb')) ############\n",
    "        df_test_matrix = transform_to_matrix(features_dset_pred) \n",
    "        prediction = model.predict(df_test_matrix)\n",
    "        pb_1 = prediction[:, 1].reshape(-1, 1)\n",
    "        prediction = np.asarray([np.argmax(line) for line in prediction]).reshape(-1, 1)\n",
    "        if n_predictions.shape[1] == 0 :\n",
    "            n_predictions = prediction\n",
    "            n_pb_1 = pb_1\n",
    "        else:\n",
    "            n_predictions = np.concatenate((n_predictions , prediction), axis = 1)\n",
    "            n_pb_1 = np.concatenate((n_pb_1, pb_1), axis = 1)\n",
    "\n",
    "    for row in range(len(n_predictions)):\n",
    "        line = n_predictions[row, :].tolist()\n",
    "        line_pb1 = n_pb_1[row, :].tolist()\n",
    "\n",
    "        mean_.append(mean(line))\n",
    "        stdev_.append(stdev(line))\n",
    "        pb1_mean.append(mean(line_pb1))\n",
    "        pb1_stdev.append(stdev(line_pb1))\n",
    "\n",
    "    mean_stdev = pd.DataFrame( { 'mean' : mean_ , 'stdev' : stdev_ , 'Prob1_mean': pb1_mean , 'Prob1_stdev' : pb1_stdev})\n",
    "    mean_stdev.to_csv(\"mean_stdev_.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022185ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 181.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.64it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pathways :  1\n",
      "Total number of reactions  :  7\n",
      "Transforming into pathway features...\n",
      "Encodining features for the Test set......\n",
      "<HDF5 dataset \"data\": shape (1, 40984), type \"<i2\">\n",
      "Predicting n times...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:02,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean - Stdev stats is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "no_of_rxns_thres = 10\n",
    "\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-trdf', '--train_data_file', required=True, type=str)\n",
    "parser.add_argument('-trsf', '--train_score_file', required=True, type=str)\n",
    "parser.add_argument('-ttdf', '--test_data_file', required=True, type=str)\n",
    "parser.add_argument('-ttsf', '--test_score_file', required=True, type=str)\n",
    "args = parser.parse_args()\n",
    "train_data_file =  args.train_data_file\n",
    "train_score_file = args.train_score_file\n",
    "test_data_file = args.test_data_file\n",
    "test_score_file = args.test_score_file\n",
    "\"\"\"\n",
    "\n",
    "# Load training and test data\n",
    "if not os.path.exists('models/data_train.h5'):\n",
    "    sys.exit('models/data_train.h5 not found')\n",
    "f=h5py.File('models/data_train.h5', \"r\")\n",
    "features_dset_train = f[\"data\"]\n",
    "\n",
    "data_test   = 'reactions_all.csv'\n",
    "scores_test = 'pathways_all.csv'\n",
    "if not os.path.exists(data_test):\n",
    "    sys.exit('reactions_features not found')\n",
    "else:\n",
    "    data_test = pd.read_csv(data_test)\n",
    "if not os.path.exists(scores_test):\n",
    "    sys.exit('pathways_features not found')\n",
    "else:\n",
    "    scores_test = pd.read_csv(scores_test)   \n",
    "print(\"Number of pathways : \",len(scores_test))\n",
    "print(\"Total number of reactions  : \", len(data_test))\n",
    "\n",
    "# Encoding and prediction\n",
    "df_test = transform_into_pathway_features(data_test, scores_test, False)\n",
    "features_dset_pred  = features_encoding(df_test, \"predict\")\n",
    "score_prediction(features_dset_train, features_dset_pred)\n",
    "print(\"Mean - Stdev stats is saved\")\n",
    "\n",
    "f.close()\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9cf5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ceb19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
